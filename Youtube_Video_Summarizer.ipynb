{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zS-0gL3NA-mk"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install youtube-transcript-api\n",
        "!pip install google-generativeai\n",
        "!pip install litellm\n",
        "!pip install sentence_transformers\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd0knZUiC0tp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIV2GHdJBQDZ"
      },
      "source": [
        "## Get YouTube video URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM_KHguHA8YO"
      },
      "outputs": [],
      "source": [
        "def get_video_transcript():\n",
        "  video_id = input(\"Enter YouTube video ID: \")\n",
        "  transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "  # Convert Python list of dictionaries to a JSON string\n",
        "  transcript = json.dumps(transcript)\n",
        "  # Parse the JSON string back into a Python list of dictionaries\n",
        "  transcript = json.loads(transcript)\n",
        "  # Combine the text values from the list of dictionaries\n",
        "  combinedText = ' '.join((obj['text']+\"\\n\") for obj in transcript).strip()\n",
        "  return combinedText\n",
        "\n",
        "\n",
        "video_transcript = get_video_transcript()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzyHuP6rCm0_"
      },
      "source": [
        "## Chunk up video transcript into smaller documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBcOtm29ClEv"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_transcript = text_splitter.split_text(video_transcript)\n",
        "texts = [Document(page_content=t) for t in split_transcript]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aAyCMLGCr-D"
      },
      "outputs": [],
      "source": [
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwu_t7kdFUWR"
      },
      "source": [
        "## Generate a summary using a chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB1Vr_hXF4wS"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, openai_api_key=\"YOUR_API_KEY\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "chain.run(docs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}